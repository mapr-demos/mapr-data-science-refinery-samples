{"paragraphs":[{"text":"%md\n\n\n### [Apache Pig](http://pig.apache.org/) is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. The salient property of Pig programs is that their structure is amenable to substantial parallelization, which in turns enables them to handle very large data sets.\n\nPig's language layer currently consists of a textual language called Pig Latin, which has the following key properties:\n\n* Ease of programming. It is trivial to achieve parallel execution of simple, \"embarrassingly parallel\" data analysis tasks. Complex tasks comprised of multiple interrelated data transformations are explicitly encoded as data flow sequences, making them easy to write, understand, and maintain.\n* Optimization opportunities. The way in which tasks are encoded permits the system to optimize their execution automatically, allowing the user to focus on semantics rather than efficiency.\n* Extensibility. Users can create their own functions to do special-purpose processing.\n","user":"mapr","dateUpdated":"2019-04-30T04:59:16-0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3><a href=\"http://pig.apache.org/\">Apache Pig</a> is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. The salient property of Pig programs is that their structure is amenable to substantial parallelization, which in turns enables them to handle very large data sets.</h3>\n<p>Pig&rsquo;s language layer currently consists of a textual language called Pig Latin, which has the following key properties:</p>\n<ul>\n  <li>Ease of programming. It is trivial to achieve parallel execution of simple, &ldquo;embarrassingly parallel&rdquo; data analysis tasks. Complex tasks comprised of multiple interrelated data transformations are explicitly encoded as data flow sequences, making them easy to write, understand, and maintain.</li>\n  <li>Optimization opportunities. The way in which tasks are encoded permits the system to optimize their execution automatically, allowing the user to focus on semantics rather than efficiency.</li>\n  <li>Extensibility. Users can create their own functions to do special-purpose processing.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1483277502513_1156234051","id":"20170101-213142_1565013608","dateCreated":"2019-04-26T05:33:58-0700","dateStarted":"2019-04-30T04:59:16-0700","dateFinished":"2019-04-30T04:59:17-0700","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4489"},{"text":"%md\n\nThis pig tutorial use pig to do the same thing as spark tutorial. The default mode is mapreduce, you can also use other modes like local/tez_local/tez. For mapreduce mode, you need to have hadoop installed and export `HADOOP_CONF_DIR` in `zeppelin-env.sh`\n\nThe tutorial consists of 3 steps.\n\n* Use shell interpreter to download bank.csv and upload it to hdfs\n* use `%pig` to process the data\n* use `%pig.query` to query the data","user":"mapr","dateUpdated":"2019-04-30T04:59:17-0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>This pig tutorial use pig to do the same thing as spark tutorial. The default mode is mapreduce, you can also use other modes like local/tez_local/tez. For mapreduce mode, you need to have hadoop installed and export <code>HADOOP_CONF_DIR</code> in <code>zeppelin-env.sh</code></p>\n<p>The tutorial consists of 3 steps.</p>\n<ul>\n  <li>Use shell interpreter to download bank.csv and upload it to hdfs</li>\n  <li>use <code>%pig</code> to process the data</li>\n  <li>use <code>%pig.query</code> to query the data</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1483689316217_-629483391","id":"20170106-155516_1050601059","dateCreated":"2019-04-26T05:33:58-0700","dateStarted":"2019-04-30T04:59:17-0700","dateFinished":"2019-04-30T04:59:17-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4490"},{"text":"%sh\n\ncd $(mktemp -d)\nwget https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv\nif ! hadoop fs -test -e bank.csv; then\n    hadoop fs -put bank.csv .\nelse\n    echo \"bank.csv alreay in your home directory in DFS\"\nfi","user":"mapr","dateUpdated":"2019-04-30T04:59:17-0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh","fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"--2019-04-30 04:59:17--  https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.136.29\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.136.29|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 461474 (451K) [application/octet-stream]\nSaving to: 'bank.csv'\n\n     0K .......... .......... .......... .......... .......... 11%  213K 2s\n    50K .......... .......... .......... .......... .......... 22% 98.3M 1s\n   100K .......... .......... .......... .......... .......... 33%  426K 1s\n   150K .......... .......... .......... .......... .......... 44% 33.9M 0s\n   200K .......... .......... .......... .......... .......... 55%  233M 0s\n   250K .......... .......... .......... .......... .......... 66%  432K 0s\n   300K .......... .......... .......... .......... .......... 77% 44.1M 0s\n   350K .......... .......... .......... .......... .......... 88% 70.2M 0s\n   400K .......... .......... .......... .......... .......... 99%  106M 0s\n   450K                                                       100% 39.1M=0.5s\n\n2019-04-30 04:59:18 (954 KB/s) - 'bank.csv' saved [461474/461474]\n\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/opt/mapr/hadoop/hadoop-2.7.0/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/opt/mapr/lib/slf4j-log4j12-1.7.12.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nbank.csv alreay in your home directory in DFS\n"}]},"apps":[],"jobName":"paragraph_1485058437578_-1906301827","id":"20170122-121357_640055590","dateCreated":"2019-04-26T05:33:58-0700","dateStarted":"2019-04-30T04:59:17-0700","dateFinished":"2019-04-30T04:59:21-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4491"},{"text":"%pig\n\nbankText = load 'bank.csv' using PigStorage(';');\nbank = foreach bankText generate $0 as age, $1 as job, $2 as marital, $3 as education, $5 as balance; \nbank = filter bank by age != '\"age\"';\nbank = foreach bank generate (int)age, REPLACE(job,'\"','') as job, REPLACE(marital, '\"', '') as marital, (int)(REPLACE(balance, '\"', '')) as balance;\n\n-- The following statement is optional, it depends on whether your needs.\n-- store bank into 'clean_bank.csv' using PigStorage(';');\n\n\n","user":"mapr","dateUpdated":"2019-04-30T04:59:21-0700","config":{"colWidth":12,"editorMode":"ace/mode/pig","results":{},"enabled":true,"editorSetting":{"language":"pig","editOnDblClick":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1483277250237_-466604517","id":"20161228-140640_1560978333","dateCreated":"2019-04-26T05:33:58-0700","dateStarted":"2019-04-30T04:59:21-0700","dateFinished":"2019-04-30T04:59:30-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4492"},{"text":"%pig.query\n\nbank_data = filter bank by age < 30;\nb = group bank_data by age;\nforeach b generate group, COUNT($1);\n\n","user":"mapr","dateUpdated":"2019-04-30T05:00:55-0700","config":{"colWidth":4,"editorMode":"ace/mode/pig","results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"group","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"col_1","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true,"editorSetting":{"language":"pig","editOnDblClick":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"group\tcol_1\n19\t4\n20\t3\n21\t7\n22\t9\n23\t20\n24\t24\n25\t44\n26\t77\n27\t94\n28\t103\n29\t97\n"}]},"apps":[],"jobName":"paragraph_1483277250238_-465450270","id":"20161228-140730_1903342877","dateCreated":"2019-04-26T05:33:58-0700","dateStarted":"2019-04-30T04:59:30-0700","dateFinished":"2019-04-30T05:00:00-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4493"},{"text":"%pig.query\n\nbank_data = filter bank by age < ${maxAge=40};\nb = group bank_data by age;\nforeach b generate group, COUNT($1) as count;","user":"mapr","dateUpdated":"2019-04-30T05:00:00-0700","config":{"colWidth":4,"editorMode":"ace/mode/pig","results":{"0":{"graph":{"mode":"pieChart","height":300,"optionOpen":false},"helium":{}}},"enabled":true,"editorSetting":{"language":"pig","editOnDblClick":false},"fontSize":9},"settings":{"params":{"maxAge":"36"},"forms":{"maxAge":{"type":"TextBox","name":"maxAge","defaultValue":"40","hidden":false,"$$hashKey":"object:5810"}}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"group\tcount\n19\t4\n20\t3\n21\t7\n22\t9\n23\t20\n24\t24\n25\t44\n26\t77\n27\t94\n28\t103\n29\t97\n30\t150\n31\t199\n32\t224\n33\t186\n34\t231\n35\t180\n"}]},"apps":[],"jobName":"paragraph_1483277250239_-465835019","id":"20161228-154918_1551591203","dateCreated":"2019-04-26T05:33:58-0700","dateStarted":"2019-04-30T05:00:00-0700","dateFinished":"2019-04-30T05:00:28-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4494"},{"text":"%pig.query\n\nbank_data = filter bank by marital=='${marital=single,single|divorced|married}';\nb = group bank_data by age;\nforeach b generate group, COUNT($1) as count;\n\n\n","user":"mapr","dateUpdated":"2019-04-30T05:00:28-0700","config":{"colWidth":4,"editorMode":"ace/mode/pig","results":{"0":{"graph":{"mode":"scatterChart","height":300,"optionOpen":false},"helium":{}}},"enabled":true,"editorSetting":{"language":"pig","editOnDblClick":false},"fontSize":9,"runOnSelectionChange":true},"settings":{"params":{"marital":"married"},"forms":{"marital":{"type":"Select","options":[{"value":"single","$$hashKey":"object:6008"},{"value":"divorced","$$hashKey":"object:6009"},{"value":"married","$$hashKey":"object:6010"}],"name":"marital","defaultValue":"single","hidden":false,"$$hashKey":"object:5999"}}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"group\tcount\n23\t3\n24\t11\n25\t11\n26\t18\n27\t26\n28\t23\n29\t37\n30\t56\n31\t104\n32\t105\n33\t103\n34\t142\n35\t109\n36\t117\n37\t100\n38\t99\n39\t88\n40\t105\n41\t97\n42\t91\n43\t79\n44\t68\n45\t76\n46\t82\n47\t78\n48\t91\n49\t87\n50\t74\n51\t63\n52\t66\n53\t75\n54\t56\n55\t68\n56\t50\n57\t78\n58\t67\n59\t56\n60\t36\n61\t15\n62\t5\n63\t7\n64\t6\n65\t4\n66\t7\n67\t5\n68\t1\n69\t5\n70\t5\n71\t5\n72\t4\n73\t6\n74\t2\n75\t3\n76\t1\n77\t5\n78\t2\n79\t3\n80\t6\n81\t1\n83\t2\n86\t1\n87\t1\n"}]},"apps":[],"jobName":"paragraph_1483277250240_-480070728","id":"20161228-142259_575675591","dateCreated":"2019-04-26T05:33:58-0700","dateStarted":"2019-04-30T05:00:28-0700","dateFinished":"2019-04-30T05:00:55-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4495"},{"text":"%pig.query\n","user":"mapr","dateUpdated":"2019-04-30T05:00:55-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"pig","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/pig"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556620955170_549638911","id":"20190430-034235_533886700","dateCreated":"2019-04-30T03:42:35-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4496"}],"name":"Zeppelin Tutorial/Using Pig for querying data","id":"2C57UKYWR","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"sh:shared_process":[],"pig:shared_process":[],"spark:shared_process":[],"livy:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":true,"looknfeel":"default","personalizedMode":"false"},"info":{}}